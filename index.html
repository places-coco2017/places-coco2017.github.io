---
layout: default
---

<div class="home">

  <h2>Overview</h2>
    <p>
    The goal of this joint challenge of COCO and Places is to bring the question of object recognition in the context of the scene understanding. 
    </p>
    
    <p> <a href="http://mscoco.org/home/">COCO</a> is an image dataset designed to spur object detection research with a focus on full scene understanding. The annotations include: presence of objects belonging to 80 common categories, pixel-level segmentation of each object, keypoint annotations for person instances, five image captions per image. The specific tracks in the COCO Challenge 2017 are (1) object detection with bounding boxes, (2) object detection with segmentation masks, and (3)
    joint detection and person keypoint estimation. See the following COCO challenge page for the detailed information: </p>
    <p class="lead"><a href="http://mscoco.org/dataset/#overview"><img src="img/coco_challenge.png" class="img-rounded"></a>
    </p>
    <p> <a href="http://placeschallenge.csail.mit.edu/">Places Challenge</a> will host three tracks meant to complement the COCO Challenge. The data the Places Challenge 2017 are from the pixel-wise annotated image dataset <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K</a>, in which there are 20K images for training, 2K images for validation, and 3K images for testing. The three specific tracks in the Places Challenge 2017 are: (1) scene parsing, (2) instance segmentation,
    and (3) semantic boundary detection. See the following Places Challenge page for the detailed information: </p>
    <p class="lead"><a href="http://placeschallenge.csail.mit.edu/ "><img src="img/places_challenge.png" class="img-rounded"></a></p>

  <h2>Important Dates</h2>
  
    <ul>
      <li>June 25, 2017: Development kits and data are available.</li>
      <li>Sep.15, 2017: Submission deadline for Places Challenge (Check Places Challenge page for the latest news)</li>
      <li>Sep.30, 2017: Submission deadline for COCO Challenge (Check COCO challenge page for the latest news)</li>
      <li>Sept.26, 2017: Places Challenge results released.</li>
      <li>Oct.22, 2017: COCO Challenge winners notified. Results officially released at the COCO workshop.</li>
      <li>Oct.29, 2017: Joint COCO and Places Challenge Workshop (full-day) </li>
    </ul>

    <h2>Workshop Schedule</h2>

<div class="post-content">
	<table >
	      <tr><th> 9:00 </th><td> Opening Comments </td></tr>
        <tr><th> 9:10 </th><td>Detection Challenges' Results, Awards, and Talks by Champions</td>
          <td> <a href="#">Tsung-Yi Lin</a> (Cornell Tech, Google Research)</td></tr>
	      <tr><th> 10:15 </th><td>Morning Break: Coffee + Posters</td></tr>
	      <tr><th> 10:20 </th><td>Invitied Talks</td>
          <td> <a href="#">Vladlen Koltun</a> (Intel Labs)</td></tr>
        <tr><th> 10:50 </th><td>Keypoints Challenge's Results, Awards, and Talks by Champions</td>
          <td> <a href="#">Matteo Ronchi</a> (Caltech)</td></tr>
        <tr><th> 11:40 </th><td>Stuff Challenge's Results, Awards, and Talks by Champions</td>
          <td> <a href="#">Holger Caesar</a> (University of Edinburgh)</td></tr>
        <tr><th> 12:30 </th><td> Lunch </td></tr>
        <tr><th> 14:00 </th><td>Invited Talk</td>
          <td> <a href="#">Raquel Urtasun</a> (Uber ATG Toronto, University of Toronto, Vector Institute)</td></tr>
        <tr><th> 14:30 </th><td>Places Challenge Results, Awards, and Talks by Champions</td>
          <td> <a href="#">Bolei Zhou (MIT), Hang Zhao (MIT)</a></td></tr>
        <tr><th> 15:45 </th><td>Afternoon Break: Coffee + Posters</td></tr>
        <tr><th> 16:15 </th><td>Discussion Panel & Announcements</td>
          <td> <a href="#">Bolei Zhou (MIT), Hang Zhao (MIT), Genevieve Patterson (MSR NE)</a></td></tr>
	    </table>
  
        <div class="organizers">
	    <h2> Invited Speakers </h2>
            <div class="row profile-row">     
                <div class="col-md-6">
                  <div class="row">
		    <a name="raquel"></a>
                        <div class="col-sm-4 img-center"><img class="img-responsive photo-size img-rounded" src="http://www.cs.toronto.edu/~urtasun/raquel_uoft.jpg"></div>
                        <div class="col-sm-8">
                            <h3 class="section-heading"><a class="theme-link-org" href="http://www.cs.toronto.edu/~urtasun/">Raquel Urtasun</a></h3>
                            <p class="lead">
                                University of Toronto, Vector Institute, Uber ATG Toronto
                            </p>
			    <p> <i>"Raquel Urtasun is the Head of Uber ATG Toronto. She is also an Associate Professor in the Department of Computer Science at the University of Toronto, a Canada Research Chair in Machine Learning and Computer Vision and a co-founder of the Vector Institute for AI. Prior to this, she was an Assistant Professor at the Toyota Technological Institute at Chicago (TTIC), an academic computer science institute affiliated with the University of Chicago. She was also a visiting professor at ETH Zurich during the spring semester of 2010. She received her Bachelors degree from Universidad Publica de Navarra in 2000, her Ph.D. degree from the Computer Science department at Ecole Polytechnique Federal de Lausanne (EPFL) in 2006 and did her postdoc at MIT and UC Berkeley. She is a world leading expert in machine perception for self-driving cars. Her research interests include machine learning, computer vision, robotics and remote sensing. Her lab was selected as an NVIDIA NVAIL lab. She is a recipient of an NSERC EWR Steacie Award, an NVIDIA Pioneers of AI Award, a Ministry of Education and Innovation Early Researcher Award, three Google Faculty Research Awards, an Amazon Faculty Research Award, a Connaught New Researcher Award and a Best Paper Runner up Prize awarded at the Conference on Computer Vision and Pattern Recognition (CVPR). She is also Program Chair of CVPR 2018, an Editor of the International Journal in Computer Vision (IJCV) and has served as Area Chair of multiple machine learning and vision conferences (i.e., NIPS, UAI, ICML, ICLR, CVPR, ECCV)." </i></p>
                            <hr style="border-color: #939393; width: 50px" align="left">
                        </div>
                    </div>
                </div>
	    </div>

	    <div class="row profile-row">     
		<div class="col-md-6">
                  <div class="row">
		    <a name="vladlen"></a>
                        <div class="col-sm-4 img-center"><img class="img-responsive photo-size img-rounded" src="http://vladlen.info/images/vladlen2.jpg"></div>
                        <div class="col-sm-8">
                            <h3 class="section-heading"><a class="theme-link-org" href="http://vladlen.info/">Vladlen Koltun</a></h3>
                            <p class="lead">Intel Labs
				</p>
			    <p><i>I direct a basic research lab at Intel Labs. We are based in two locations: Santa Clara, California and Munich, Germany. We are hiring interns, postdocs, and staff researchers in both locations. We are broadly interested in visual computing and intelligent systems. Our work is usually published in computer vision, machine learning, and computer graphics conferences.</i></p>
			<hr style="border-color: #939393; width: 50px" align="left">
                        </div>
                    </div>
		</div>
             </div>
    </div>
	    </div>
</div>
