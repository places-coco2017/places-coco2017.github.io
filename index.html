<!DOCTYPE html>
<head>
  <meta charset="UTF-8">
  <title>COCO + Places 2017 | ICCV 2017</title>
  <link rel="stylesheet" href="https://ajax.googleapis.com/ajax/libs/jqueryui/1.12.1/themes/smoothness/jquery-ui.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" />
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" />
  <link rel="stylesheet" href="css/cocostyles.css" />
  <link rel="stylesheet" href="css/main.css" />
</head>
<body>
  <div class="iccvheader">
    <div class="iccvheadercontent">
      <div class="iccvtitle shadow">COCO + Places 2017</div>
      <div class="iccvsubtitle shadow">Joint Workshop of the COCO and Places Challenges at ICCV 2017</div>
    </div>
  </div>

  <div id="content">
    <h2>1. Overview</h2>
    <p>The goal of the joint COCO and Places Challenge is to study object recognition in the context of scene understanding.</p>

    <h2>2. COCO Challenges</h2>
    <p><a href="http://cocodataset.org/">COCO</a> is an image dataset designed to spur object detection research with a focus on detecting objects in context. The annotations include pixel-level segmentation of object belonging to 80 categories, keypoint annotations for person instances, stuff segmentations for 91 categories, and five image captions per image. The specific tracks in the COCO 2017 Challenges are (1) object detection with bounding boxes and segmentation masks, (2) joint detection and person keypoint estimation, and (3) stuff segmentation. We describe each next.</p>

    <div class="subsection">
      <h3>2.1. COCO Detection Challenge</h3>
      <p align="center"><a href="http://cocodataset.org/#detections-challenge2017"><img src="http://cocodataset.org/images/detections-challenge-splash.png" class="wide" /></a></p>
      <p>The COCO 2017 Detection Challenge is designed to push the state of the art in object detection forward. Teams are encouraged to compete in either (or both) of two object detection challenges: using bounding box output or object segmentation output.  For full details of this task please see the <a href="http://cocodataset.org/#detections-challenge2017">COCO Detection Challenge</a> page.</p>
    </div>

    <div class="subsection">
      <h3>2.2. COCO Keypoint Challenge</h3>
      <p align="center"><a href="http://cocodataset.org/#keypoints-challenge2017"><img src="http://cocodataset.org/images/keypoints-challenge-splash.png" class="wide" /></a></p>
      <p>The COCO 2017 Keypoint Challenge requires localization of person keypoints in challenging, uncontrolled conditions. The keypoint challenge involves simultaneously detecting people <i>and</i> localizing their keypoints (person locations are <i>not</i> given at test time). For full details of this task please see the <a href="http://cocodataset.org/#keypoints-challenge2017">COCO Keypoints Challenge</a> page.</p>
    </div>

    <div class="subsection">
      <h3>2.3. COCO Stuff Challenge</h3>
      <p align="center"><a href="http://cocodataset.org/#stuff-challenge2017"><img src="http://cocodataset.org/images/stuff-challenge-splash.png" class="wide" /></a></p>
      <p>The COCO 2017 Stuff Segmentation Challenge is designed to push the state of the art in semantic segmentation of <i>stuff</i> classes. Whereas the COCO 2017 Detection Challenge addresses <i>thing</i> classes (person, car, elephant), this challenge focuses on <i>stuff</i> classes (grass, wall, sky). For full details of this task please see the <a href="http://cocodataset.org/#stuff-challenge2017">COCO Stuff Challenge</a> page.</p>
    </div>

    <h3>3. Places Challenges</h3>
    <div class="subsection">
      <p><a href="http://placeschallenge.csail.mit.edu/ "><img src="img/places_challenge.png" class="wide"></a></p>
      <p>The <a href="http://placeschallenge.csail.mit.edu/">Places Challenge</a> will host three tracks meant to complement the COCO Challenges. The data for the 2017 Places Challenge is from the pixel-wise annotated image dataset <a href="http://groups.csail.mit.edu/vision/datasets/ADE20K/">ADE20K</a>, in which there are 20K images for training, 2K validation images, and 3K testing images. The three specific tracks in the Places Challenge 2017 are: (1) scene parsing, (2) instance segmentation, and (3) semantic boundary detection. See the <a href="http://placeschallenge.csail.mit.edu/">Places Challenge Page</a> for detailed information. </p>
      <p><a href="http://placeschallenge.csail.mit.edu/"><img src="http://placeschallenge.csail.mit.edu/assets/images/tasks.png" class="wide"></a></p>
    </div>

    <h2>4. Challenge Dates</h2>
    <div class="json">
      <div class="jsonktxt fontBlue">September 30, 2017</div><div class="jsonvtxt"><b>Detection</b> &amp; <b>Keypoints</b> Submission deadline (11:59 PST)</div>
      <div class="jsonktxt fontBlue">September 30, 2017</div><div class="jsonvtxt">[Extended] <b>Places</b> Submission deadline (11:59 PST)</div>
      <div class="jsonktxt fontBlue">October 8, 2017</div><div class="jsonvtxt">[Extended] <b>Stuff</b> Submission deadline (11:59 PST)</div>
      <div class="jsonktxt">October 15, 2017</div><div class="jsonvtxt">Challenge winners notified</div>
      <div class="jsonktxt">October 29, 2017</div><div class="jsonvtxt">Winners present at ICCV 2017 Workshop</div>
    </div>

    <h2>5. Workshop Schedule - 10.29.2017</h2>
    <table class="table">
      <tr>
        <th>9:00</th>
        <td>Opening Comments</td>
        <td></td>
      </tr>
      <tr>
        <th>9:10</th>
        <td>Detection Challenge Track</td>
        <td>Tsung-Yi Lin (Cornell Tech, Google Research)</td>
      </tr>
      <tr>
        <th>10:10</th>
        <td>Morning Break: Coffee + Posters</td>
        <td></td>
      </tr>
      <tr>
        <th>10:40</th>
        <td>Keypoints Challenge Track</td>
        <td>Matteo Ronchi (Caltech)</td>
      </tr>
      <tr>
        <th>11:20</th>
        <td>Stuff Challenge Track</td>
        <td>Holger Caesar (University of Edinburgh)</td>        
      </tr>
      <tr>
        <th>12:00</th>
        <td>Invited Talk</td>
        <td><a href="#vladlen">Vladlen Koltun</a> (Intel Labs)</td>        
      </tr>
      <tr>
        <th>12:30</th>
        <td>Lunch</td>
        <td></td>
      </tr>
      <tr><th>14:00</th>
        <td>Invited Talk</td>
        <td><a href="#raquel">Raquel Urtasun</a> (University of Toronto, Vector Institute, Uber ATG Toronto)</td>
      </tr>
      <tr>
        <th>14:30</th>
        <td>Places Challenge Track</td>
        <td>Bolei Zhou (MIT), Hang Zhao (MIT)</td>
      </tr>
      <tr>
        <th>15:45</th>
        <td>Afternoon Break: Coffee + Posters</td>
        <td></td>
      </tr>
      <tr>
        <th>16:15</th>
        <td>Discussion Panel</td>
        <td>Bolei Zhou (MIT), Hang Zhao (MIT), Genevieve Patterson (MSR NE)</td>
      </tr>
    </table>
    <p class="fontSubtle"><b>Note: schedule subject to change.</b></p>

    <h2>6. Invited Speakers </h2>
    <div>
      <a name="raquel"></a>
      <div class="speakerimg">
        <img class="wide img-rounded" src="http://www.cs.toronto.edu/~urtasun/raquel_uoft.jpg">
      </div>
      <div class="speakerbio">
        <h3><a href="http://www.cs.toronto.edu/~urtasun/" target="_blank">Raquel Urtasun</a></h3>
        <p>University of Toronto, Vector Institute, Uber ATG Toronto</p>
        <p><i>Raquel Urtasun is the Head of Uber ATG Toronto. She is also an Associate Professor in the Department of Computer Science at the University of Toronto, a Canada Research Chair in Machine Learning and Computer Vision and a co-founder of the Vector Institute for AI.</i></p>
      </div>
    </div>
    <div>
      <a name="vladlen"></a>
      <div class="speakerimg">
        <img class="wide img-rounded" src="http://vladlen.info/images/vladlen2.jpg">
      </div>
      <div class="speakerbio">
        <h3><a href="http://vladlen.info/" target="_blank">Vladlen Koltun</a></h3>
        <p>Intel Labs</p>
        <p><i>I direct a basic research lab at Intel Labs. We are based in two locations: Santa Clara, California and Munich, Germany. We are hiring interns, postdocs, and staff researchers in both locations. We are broadly interested in visual computing and intelligent systems. Our work is usually published in computer vision, machine learning, and computer graphics conferences.</i></p>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <p>
      Workshop questions?<br/>
      Email <a href="info@cocodataset.org">info@cocodataset.org</a> or <a href="mailto:bzhou@csail.mit.edu">Bolei Zhou</a><br/>
      COCO Team and MIT Vision Group
    </p>
  </footer>
</body>
